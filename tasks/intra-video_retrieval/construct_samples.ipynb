{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "segments = json.load(open('../../data/annotation.json'))\n",
    "animations = json.load(open('../../data/steps.json'))\n",
    "splits = json.load(open('../split.json'))\n",
    "train_set = splits['train']\n",
    "val_set = splits['val']\n",
    "test_set = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "fps = 29.97\n",
    "min_d = int(30 * fps) # minimal distance from the candidates to the query\n",
    "d_b = int(15 * fps) # minimal distance from the candidates and query to the step boundary\n",
    "\n",
    "def valid(x, L1, L2):\n",
    "    # if there is valid distance d when the segment lengths are L1 and L2, and the mid point is x\n",
    "    if x + d_b >= L1 or x < 0:\n",
    "        return False\n",
    "    mn = max(min_d, L1 + d_b - x) # minimal possible d\n",
    "    mx = min(x - d_b, L1 + L2 - 1 - x - d_b) # maximal possible d\n",
    "    return mn <= mx\n",
    "\n",
    "def get_samples(L1, L2, M):\n",
    "    samples = []\n",
    "    valid_x = []\n",
    "    for x in range(0, L1):\n",
    "        if valid(x, L1, L2):\n",
    "            valid_x.append(x)\n",
    "    for i in range(M):\n",
    "        m = len(valid_x) * i // M\n",
    "        m = valid_x[m]\n",
    "        try:\n",
    "            d = random.randint(max(min_d, L1 + d_b - m), min(m - d_b, L1 + L2 - 1 - m - d_b))\n",
    "        except:\n",
    "            print(L1, L2, m)\n",
    "        l = m - d\n",
    "        r = m + d\n",
    "        assert m >= d_b and m + d_b < L1\n",
    "        assert d >= min_d\n",
    "        assert l >= d_b and l + d_b < L1\n",
    "        assert r - d_b >= L1 and r + d_b < L1 + L2\n",
    "        samples.append((m, l, r))\n",
    "    return samples\n",
    "\n",
    "def construct_(segments, n_samples):\n",
    "    n_samples_tmp = n_samples\n",
    "    data = []\n",
    "    candidates = []\n",
    "    caps = []\n",
    "    for i in range(len(segments) - 1):\n",
    "        if segments[i]['label'] > -1 and segments[i+1]['label'] > -1:\n",
    "            X1, X2 = [], []\n",
    "            L1 = segments[i]['frame_end'] - segments[i]['frame_start']\n",
    "            L2 = segments[i+1]['frame_end'] - segments[i+1]['frame_start']\n",
    "            for x in range(0, L1):\n",
    "                if valid(x, L1, L2):\n",
    "                    X1.append(x)\n",
    "            for x in range(0, L2):\n",
    "                if valid(x, L2, L1):\n",
    "                    X2.append(x)\n",
    "            if len(X1) > 0 and len(X2) > 0:\n",
    "                candidates.append(i)\n",
    "                caps.append(min(len(X1), len(X2)) * 2)\n",
    "\n",
    "    # Evenly allocate the samples to each adjacent segment pair\n",
    "    n_samples_ = [0] * len(candidates)\n",
    "    while n_samples > 0:\n",
    "        for i in range(len(candidates)):\n",
    "            if n_samples > 0 and caps[i] > 0:\n",
    "                if segments[candidates[i]]['label'] == 0 or segments[candidates[i]+1]['label'] == 0:\n",
    "                    # There are too many irrelevant segments, so we need to draw samples from them less\n",
    "                    if random.randint(1, 100) == 1:\n",
    "                        n_samples_[i] += 1\n",
    "                        n_samples -= 1\n",
    "                        caps[i] -= 1\n",
    "                else:\n",
    "                    n_samples_[i] += 1\n",
    "                    n_samples -= 1\n",
    "                    caps[i] -= 1\n",
    "    for i, M  in zip(candidates, n_samples_):\n",
    "        hf = M // 2\n",
    "\n",
    "        # Half samples with the query timestamp in the former segment\n",
    "        samples = get_samples(segments[i]['frame_end'] - segments[i]['frame_start'], segments[i+1]['frame_end'] - segments[i+1]['frame_start'], hf)\n",
    "        for m, l, r in samples:\n",
    "            data.append({\n",
    "                'query_frame_index': segments[i]['frame_start'] + m,\n",
    "                'candidates': [\n",
    "                    segments[i]['frame_start'] + l,\n",
    "                    segments[i]['frame_start'] + r\n",
    "                ] # the first one is positive and the second one is negative\n",
    "            })\n",
    "        \n",
    "        # Half samples with the query timestamp in the latter segment\n",
    "        samples = get_samples(segments[i+1]['frame_end'] - segments[i+1]['frame_start'], segments[i]['frame_end'] - segments[i]['frame_start'], M - hf)\n",
    "        L = segments[i+1]['frame_end'] - segments[i]['frame_start'] - 1\n",
    "        for m, l, r in samples:\n",
    "            data.append({\n",
    "                'query_frame_index': segments[i]['frame_start'] + L - m,\n",
    "                'candidates': [\n",
    "                    segments[i]['frame_start'] + L - l,\n",
    "                    segments[i]['frame_start'] + L - r\n",
    "                ] # the first one is positive and the second one is negative\n",
    "            })\n",
    "    for i in range(len(data)):\n",
    "        # randomly shuffle the positive and the negative\n",
    "        label = random.randint(0, 1)\n",
    "        if label == 1:\n",
    "            cans = [data[i]['candidates'][1], data[i]['candidates'][0]]\n",
    "        else:\n",
    "            cans = data[i]['candidates']\n",
    "        data[i]['candidate_frame_indices'] = cans\n",
    "        data[i].pop('candidates')\n",
    "        data[i]['label'] = label\n",
    "        data[i]['id'] = i\n",
    "    assert len(data) == n_samples_tmp\n",
    "    return data\n",
    "\n",
    "def construct(segments, dates, n_samples):\n",
    "    '''\n",
    "        Construct `n_samples` samples from each video listed in `dates`\n",
    "    '''\n",
    "    data = {}\n",
    "    for date in dates:\n",
    "        print('Processing:', date)\n",
    "        data[date] = construct_(segments[date], n_samples)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 01152020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 01252020\n",
      "Processing: 01272021\n",
      "Processing: 02012021\n",
      "Processing: 03232022\n",
      "Processing: 06162021\n",
      "Processing: 10062019\n",
      "Processing: 11222019\n",
      "Processing: 12022021\n",
      "Processing: 12032022\n"
     ]
    }
   ],
   "source": [
    "# #####################################\n",
    "#             Warning                #\n",
    "# As randomness is involved, please  #\n",
    "# modify the output file name before #\n",
    "# running this code. Otherwise, it   #\n",
    "# will overwrite the training set.   # \n",
    "# #####################################\n",
    "\n",
    "# Construct training set\n",
    "data = construct(segments, train_set, 2000)\n",
    "with open('train.json', 'w') as f:\n",
    "    f.write(json.dumps(data, indent=4))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 03152022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 11152022\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#             Warning               #\n",
    "# As randomness is involved, please #\n",
    "# don't run this to overwrite the   #\n",
    "# original validation set.          #\n",
    "#####################################\n",
    "\n",
    "# Construct validation set\n",
    "data = construct(segments, val_set, 2000)\n",
    "with open('val.json', 'w') as f:\n",
    "    f.write(json.dumps(data, indent=4))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 02282021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 06092023\n",
      "Processing: 06262020\n",
      "Processing: 09122021\n",
      "Processing: 11152019\n",
      "Processing: 12022019\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#             Warning               #\n",
    "# As randomness is involved, please #\n",
    "# don't run this to overwrite the   #\n",
    "# original test set.                #\n",
    "#####################################\n",
    "\n",
    "# Construct test set\n",
    "data = construct(segments, test_set, 2000)\n",
    "with open('test.json', 'w') as f:\n",
    "    f.write(json.dumps(data, indent=4))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
